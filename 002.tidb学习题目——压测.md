# tidb压测题目

分值：300  题目描述：  使用 sysbench、go-ycsb 和 go-tpc 分别对  TiDB 进行测试并且产出测试报告。  测试报告需要包括以下内容： 

* 部署环境的机器配置(CPU、内存、磁盘规格型号)

* 拓扑结构(TiDB、TiKV 各部署于哪些节点) 

* 调整过后的 TiDB 和 TiKV 配置 

*  测试输出结果记录

* 关键指标的监控截图：

  ​		TiDB Query Summary 中的 qps 与 duration 	   

  ​		TiKV Details 面板中 Cluster 中各 server 的 CPU 以及 QPS 指标 	    

  ​		TiKV Details 面板中 grpc 的 qps 以及 duration

* 输出：写出你对该配置与拓扑环境和 workload 下 TiDB 集群负载的分析，提出你认为的 TiDB 的性能的瓶颈所在(能提出大致在哪个模块即 可)  

* 截止时间：下周二（8.25）24:00:00(逾期提交不给分)



# 部署环境的机器配置(CPU、内存、磁盘规格型号)

## 两台vmwar虚拟机，各8C,16G

```shell
[root@tbase01 ~]# lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                8
On-line CPU(s) list:   0-7
Thread(s) per core:    1
Core(s) per socket:    8
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 44
Model name:            Intel(R) Xeon(R) CPU           E5645  @ 2.40GHz
Stepping:              2
CPU MHz:               2393.703
BogoMIPS:              4788.00
Hypervisor vendor:     VMware
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              12288K
NUMA node0 CPU(s):     0-7
[root@tbase01 ~]# 
[root@tbase01 ~]# free -h
              total        used        free      shared  buff/cache   available
Mem:            15G        345M         13G        628M        2.0G         14G
Swap:            0B          0B          0B
[root@tbase01 ~]# 
```

## 各分配了一块150G虚拟硬盘

```shell
[root@tbase01 ~]# cat /proc/scsi/scsi
Attached devices:
Host: scsi0 Channel: 00 Id: 00 Lun: 00
  Vendor: VMware   Model: Virtual disk     Rev: 2.0 
  Type:   Direct-Access                    ANSI  SCSI revision: 06
Host: scsi3 Channel: 00 Id: 00 Lun: 00
  Vendor: NECVMWar Model: VMware SATA CD00 Rev: 1.00
  Type:   CD-ROM                           ANSI  SCSI revision: 05
[root@tbase01 ~]# 
[root@tbase02 ~]# lsscsi 
[0:0:0:0]    disk    VMware   Virtual disk     2.0   /dev/sda 
[3:0:0:0]    cd/dvd  NECVMWar VMware SATA CD00 1.00  /dev/sr0 
[root@tbase02 ~]# hdparm -i /dev/sda
/dev/sda:
SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 c0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
 HDIO_GET_IDENTITY failed: Invalid argument
[root@tbase02 ~]# 
```

## 测试硬盘

###  随机同步写入性能 

```SHELL
[root@tbase02 ~]# time dd if=/dev/zero of=/tmp/test bs=8k count=51200 oflag=dsync 
51200+0 records in
51200+0 records out
419430400 bytes (419 MB) copied, 138.307 s, 3.0 MB/s

real	2m18.315s
user	0m0.106s
sys	0m4.403s
[root@tbase02 ~]# 
```

### 批量写入性能

```shell
[root@tbase02 ~]# time dd if=/dev/zero of=/tmp/test4 bs=8k count=51200 conv=fsync
51200+0 records in
51200+0 records out
419430400 bytes (419 MB) copied, 2.04958 s, 205 MB/s

real	0m2.051s
user	0m0.011s
sys	0m0.541s
[root@tbase02 ~]#
```

### 读出性能 

```shell
[root@tbase02 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:          15886        1324        4974        1008        9587       13194
Swap:             0           0           0
[root@tbase02 ~]# echo 3 > /proc/sys/vm/drop_caches 
[root@tbase02 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:          15886        1323       13386        1008        1176       13290
Swap:             0           0           0
[root@tbase02 ~]# time dd if=/tmp/test of=/dev/null bs=8k count=51200 oflag=dsync 
51200+0 records in
51200+0 records out
419430400 bytes (419 MB) copied, 2.06349 s, 203 MB/s

real	0m2.092s
user	0m0.010s
sys	0m0.342s
[root@tbase02 ~]# 
```



## 拓扑结构(TiDB、TiKV 各部署于哪些节点) 

```shell
[root@tbase02 ~]# cat wei_tidb_241and242.yaml 
# # Global variables are applied to all deployments and used as the default value of
# # the deployments if a specific deployment value is missing.
global:
 user: "tidb"
 ssh_port: 22
 deploy_dir: "/tidb-deploy830"
 data_dir: "/tidb-data830"
    
# # Monitored variables are applied to all the machines.
monitored:
 node_exporter_port: 19100
 blackbox_exporter_port: 19115
    
server_configs:
 tidb:
   log.slow-threshold: 300
 tikv:
   readpool.storage.use-unified-pool: false
   readpool.coprocessor.use-unified-pool: true
 pd:
   replication.enable-placement-rules: true
 tiflash:
   logger.level: "info"
    
pd_servers:
 - host: 192.168.1.242
    
tidb_servers:
 - host: 192.168.1.241
 - host: 192.168.1.242
    
tikv_servers:
 - host: 192.168.1.241
   port: 30160
   status_port: 30180
    
 - host: 192.168.1.242
   port: 30161
   status_port: 30181
    
tiflash_servers:
 - host: 192.168.1.242
    
monitoring_servers:
 - host: 192.168.1.242
    
grafana_servers:
 - host: 192.168.1.242
[root@tbase02 ~]# 
[root@tbase02 ~]# tiup cluster destroy tidb-test
[root@tbase02 ~]# tiup cluster deploy tidb830 v4.0.0 ./wei_tidb_241and242.yaml --user root -p
Starting component `cluster`: /root/.tiup/components/cluster/v1.0.9/tiup-cluster deploy tidb830 v4.0.0 ./wei_tidb_241and242.yaml --user root -p
Please confirm your topology:
tidb Cluster: tidb830
tidb Version: v4.0.0
Type        Host           Ports                            OS/Arch       Directories
----        ----           -----                            -------       -----------
pd          192.168.1.241  2379/2380                        linux/x86_64  /tidb-deploy830/pd-2379,/tidb-data830/pd-2379
tikv        192.168.1.241  30160/30180                      linux/x86_64  /tidb-deploy830/tikv-30160,/tidb-data830/tikv-30160
tikv        192.168.1.242  30161/30181                      linux/x86_64  /tidb-deploy830/tikv-30161,/tidb-data830/tikv-30161
tidb        192.168.1.241  4000/10080                       linux/x86_64  /tidb-deploy830/tidb-4000
tidb        192.168.1.242  4000/10080                       linux/x86_64  /tidb-deploy830/tidb-4000
tiflash     192.168.1.242  9000/8123/3930/20170/20292/8234  linux/x86_64  /tidb-deploy830/tiflash-9000,/tidb-data830/tiflash-9000
prometheus  192.168.1.242  9090                             linux/x86_64  /tidb-deploy830/prometheus-9090,/tidb-data830/prometheus-9090
grafana     192.168.1.242  3000                             linux/x86_64  /tidb-deploy830/grafana-3000
Attention:
    1. If the topology is not what you expected, check your yaml file.
    2. Please confirm there is no port/directory conflicts in same host.
Do you want to continue? [y/N]:  y
```

```shell
# 这个etcd是什么还不清楚，以后再研究吧.好像是和tbase冲突了
Destroying instance 192.168.1.241
retry error: operation timed out after 1m0s
192.168.1.241 error destroying pd: timed out waiting for port 2379 to be stopped after 1m0s

Error: failed to destroy pd: 192.168.1.241 error destroying pd: timed out waiting for port 2379 to be stopped after 1m0s: timed out waiting for port 2379 to be stopped after 1m0s

Verbose debug logs has been written to /root/logs/tiup-cluster-debug-2020-08-22-10-53-15.log.
Error: run `/root/.tiup/components/cluster/v1.0.9/tiup-cluster` (wd:/root/.tiup/data/S8MGkCu) failed: exit status 1
[root@tbase02 ~]#
[root@tbase01 bin]# mv /usr/bin/etcd /tmp
[root@tbase01 bin]# ps -ef |grep etcd
etcd      9925     1  2 10:57 ?        00:00:04 /usr/bin/etcd --name=etcd1 --data-dir=/data/etcd_data --listen-client-urls=http://0.0.0.0:2379,http://0.0.0.0:4001
root     13468  2142  0 11:00 pts/1    00:00:00 grep --color=auto etcd
[root@tbase01 bin]# kill -9 9925
[root@tbase01 bin]# ps -ef |grep etcd
tbase    13721  2373  0 11:01 ?        00:00:00 bash -c export ETCDCTL_API=3;etcdctl endpoint health --endpoints=192.168.1.241:2379
tbase    13722 13721  1 11:01 ?        00:00:00 etcdctl endpoint health --endpoints=192.168.1.241:2379
root     13757  2142  0 11:01 pts/1    00:00:00 grep --color=auto etcd
[root@tbase01 bin]# cp /tmp/etcd /usr/bin/etcd 
# 结果：确实是tbase的etcd和tidb-pd冲突，将pd移动到第二台机器上正常。
[root@tbase02 ~]# tiup cluster start tidb830
```

```shell
[root@tbase02 ~]# tiup cluster display tidb830
Starting component `cluster`: /root/.tiup/components/cluster/v1.0.9/tiup-cluster display tidb830
tidb Cluster: tidb830
tidb Version: v4.0.0
ID                   Role        Host           Ports                            OS/Arch       Status   Data Dir                       Deploy Dir
--                   ----        ----           -----                            -------       ------   --------                       ----------
192.168.1.242:3000   grafana     192.168.1.242  3000                             linux/x86_64  Up       -                              /tidb-deploy830/grafana-3000
192.168.1.242:2379   pd          192.168.1.242  2379/2380                        linux/x86_64  Up|L|UI  /tidb-data830/pd-2379          /tidb-deploy830/pd-2379
192.168.1.242:9090   prometheus  192.168.1.242  9090                             linux/x86_64  Up       /tidb-data830/prometheus-9090  /tidb-deploy830/prometheus-9090
192.168.1.241:4000   tidb        192.168.1.241  4000/10080                       linux/x86_64  Up       -                              /tidb-deploy830/tidb-4000
192.168.1.242:4000   tidb        192.168.1.242  4000/10080                       linux/x86_64  Up       -                              /tidb-deploy830/tidb-4000
192.168.1.242:9000   tiflash     192.168.1.242  9000/8123/3930/20170/20292/8234  linux/x86_64  Up       /tidb-data830/tiflash-9000     /tidb-deploy830/tiflash-9000
192.168.1.241:30160  tikv        192.168.1.241  30160/30180                      linux/x86_64  Up       /tidb-data830/tikv-30160       /tidb-deploy830/tikv-30160
192.168.1.242:30161  tikv        192.168.1.242  30161/30181                      linux/x86_64  Up       /tidb-data830/tikv-30161       /tidb-deploy830/tikv-30161
[root@tbase02 ~]# 
```



## 测试环境小结

- 配置环境说明

| 项目           |                                                              |
| -------------- | ------------------------------------------------------------ |
| 操作系统       | CentOS Linux release 7.3.1611                                |
| TiDB 版本      | TiDB-v4.0.0                                                  |
| TiDB & PD & KV | hosts1： tidb-server ，tikv-server，                         |
| TiDB & PD & KV | hosts2： tidb-server ，tikv-server，tipd-server，grafana, prometheus,  tiflash |
| TiDB 默认参数  | [log]<br/>slow-threshold = 300                               |
| TiKV 默认参数  | [readpool]<br/>[readpool.coprocessor]<br/>use-unified-pool = true<br/>[readpool.storage]<br/>use-unified-pool = false |



## 测试输出结果记录 

sysbench、go-ycsb 和 go-tpc

#### sysbench

```shell
[root@tbase02 wangwei]# sysbench --config-file=tidb.cfg oltp_point_select --tables=6 --table-size=100000 prepare
[root@tbase02 wangwei]# sysbench --config-file=tidb.cfg oltp_point_select --tables=6 --table-size=100000 run
sysbench 1.0.14 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 8
Report intermediate results every 10 second(s)
Initializing random number generator from current time


Initializing worker threads...

Threads started!

[ 10s ] thds: 8 tps: 12092.49 qps: 12092.49 (r/w/o: 12092.49/0.00/0.00) lat (ms,95%): 0.87 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 8 tps: 11755.09 qps: 11755.09 (r/w/o: 11755.09/0.00/0.00) lat (ms,95%): 0.90 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 8 tps: 11693.12 qps: 11693.12 (r/w/o: 11693.12/0.00/0.00) lat (ms,95%): 0.92 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 8 tps: 11667.91 qps: 11667.91 (r/w/o: 11667.91/0.00/0.00) lat (ms,95%): 0.92 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 8 tps: 11755.78 qps: 11755.78 (r/w/o: 11755.78/0.00/0.00) lat (ms,95%): 0.92 err/s: 0.00 reconn/s: 0.00
SQL statistics:
    queries performed:
        read:                            707485
        write:                           0
        other:                           0
        total:                           707485
    transactions:                        707485 (11790.49 per sec.)
    queries:                             707485 (11790.49 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          60.0026s
    total number of events:              707485

Latency (ms):
         min:                                    0.35
         avg:                                    0.68
         max:                                   23.46
         95th percentile:                        0.90
         sum:                               478804.31

Threads fairness:
    events (avg/stddev):           88435.6250/24.85
    execution time (avg/stddev):   59.8505/0.00

[root@tbase02 wangwei]# 
```

![1598070795488](D:\w_gopath\src\tidb830\jpg\sysbench-qps.png)



#### go-ycsb



#### go-tpc



