# 003.家庭作业Tidb 通过工具寻找瓶颈

## 测试环境

| 项目                         | 配置情况                                                     |
| ---------------------------- | ------------------------------------------------------------ |
| 操作系统硬件（2个vmwar分区） | 两台vmwar虚拟机，各8C,16G，Intel(R) Xeon(R) CPUE5645  @ 2.40GHz  CentOS Linux release 7.3.1611 |
| TiDB 版本                    | TiDB-v4.0.0                                                  |
| Host1：192.168.1.241         | hosts1： tidb-server ，tikv-server，                         |
| Host2：192.168.1.242         | hosts2： tidb-server ，tikv-server，tipd-server，grafana, prometheus,  tiflash |
| TiDB 默认参数                | [log]<br/>slow-threshold = 300                               |
| TiKV 默认参数                | [readpool]<br/>[readpool.coprocessor]<br/>use-unified-pool = true<br/>[readpool.storage]<br/>use-unified-pool = false |

## 测试脚本

在192.168.1.242上运行测试脚本和收集测试数据：

```shell
#测试
export LANG=en_US 
wwdir=`date "+%Y%m%d_%H%M%S"` 
mkdir -p /home/gocode/src/github.com/pingcap/wei_ceshi_${wwdir}
cd /home/gocode/src/github.com/pingcap/wei_ceshi_${wwdir}
#nohup sysbench --config-file=../sysbench/tidb.cfg oltp_point_select --tables=6 --table-size=100000 run &
#nohup sysbench --config-file=../sysbench/tidb.cfg  oltp_read_write --tables=6 --table-size=100000 run &
nohup sysbench --config-file=../sysbench/tidb.cfg  oltp_write_only --tables=6 --table-size=100000 run > sysbench.log 2>&1 &
sleep 20
# 收集
tikvpid=888
mkdir -p /home/gocode/src/github.com/pingcap/wei_ceshi_${wwdir}
cd /home/gocode/src/github.com/pingcap/wei_ceshi_${wwdir}
 curl http://192.168.1.241:10080/debug/zip?seconds=30 --output debug241.zip > debug241.log 2>&1 &
 curl http://192.168.1.242:10080/debug/zip?seconds=30 --output debug242.zip > debug241.log 2>&1 &
cp -r /home/gocode/src/github.com/pingcap/tidb-inspect-tools/tracing_tools/perf ./perfc
cd perfc
 sh cpu_tikv.sh $tikvpid  > cpu_tikv.log 2>&1 &
cd ..
cp -r /home/gocode/src/github.com/pingcap/tidb-inspect-tools/tracing_tools/perf ./perfd
cd perfd
 sh dot_tikv.sh $tikvpid  > doc_tikv.log 2>&1 &
cd ..
cp -r /home/gocode/src/github.com/pingcap/tidb-inspect-tools/tracing_tools/perf ./perfm
cd perfm
 sh mem.sh $tikvpid > mem_tikv.log 2>&1 &
cd ..

mkdir -p /tmp/tmpww 
cd /tmp/tmpww 
#export LANG=en_US 
#wwdir=`date "+%Y%m%d_%H%M%S"` 
mkdir os_${wwdir} 
cd os_${wwdir}

#cpu,mem,tasks,threads
vmstat 1 30 -t > vmstat_1 & 
top -d 1 -n 30 -b > top_1 & 
top -d 1 -n 30 -b -H > toph_1 & 
sar -P ALL 1 30 -t > cpuall_1 &
ps auxw --sort=%cpu  > psaux_1 &

#network
sar -n DEV 1 30 -t > sarnet_1 & 
iftop -t -L 20 -s 30 -n -N -B > iftop_1 &

#disk
iostat 1 30 -x -d -t > iostatx_1 & 
iotop -botq --iter=30 |sed "s/\(..:..:.. Total\)/\n\1/" > iotop_1 &

sleep 30
ps auxw --sort=%cpu  > psaux_2 &

#ping 10.167.112.10 -i 1 -c 30 -I 10.167.112.8
#hyptop -d 1  -b > hyptopp &
sleep 30
cd /home/gocode/src/github.com/pingcap/
cp -r /tmp/tmpww/os_${wwdir} ./wei_ceshi_${wwdir}/
tar zcf wei_ceshi_${wwdir}.tar.gz wei_ceshi_${wwdir}
```

## 测试结果分析

```shell
go tool pprof -http=:8080 profile
go tool pprof -http=:8081 heap
go tool pprof -alloc_space -http=:8082 heap
go tool pprof -contentions -http=:8083 mutex
TiKV直接查看perf目录下对应的cpu、mem结果文件。
```

TiDB和TiKV的CPU使用很低，MEM使用平缓，目前发现Disk有可能是瓶颈。

```shell
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00   147.00    0.00  255.00     0.00  2312.00    18.13     1.01    3.96    0.00    3.96   3.58  91.30
sda               0.00   177.00    0.00  318.00     0.00  2088.00    13.13     0.89    2.78    0.00    2.78   2.78  88.50
sda               0.00   168.00    0.00  299.00     0.00  1992.00    13.32     0.92    3.04    0.00    3.04   3.06  91.60
```

IO的特点是小包，QPS不高，每个IO大概延时3ms，似乎也没达到Disk的能力上限呀。

通过dd，fio等工具单独测试了一下磁盘性能：

```shell
time dd if=/dev/zero of=/tmp/test8 bs=256 count=5120 oflag=dsync
		Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
    bs=256	sda               0.00   202.00    0.00  499.00     0.00  2816.00    11.29     0.98    1.98    0.00    1.98   1.96  97.90
    bs=512	sda               0.00   189.00    0.00  453.00     0.00  2568.00    11.34     0.97    2.13    0.00    2.13   2.14  97.00
     bs=1k	sda               0.00   250.00    0.00  495.00     0.00  2980.00    12.04     0.98    1.98    0.00    1.98   1.99  98.40
     bs=2k	sda               0.00   537.00    0.00  639.00     0.00  4704.00    14.72     0.96    1.50    0.00    1.50   1.51  96.30
     bs=2k	sda               0.00  1193.00    0.00  892.00     0.00  8324.00    18.66     0.96    1.08    0.00    1.08   1.08  96.10
     bs=8k	sda               0.00  1319.00    0.00  983.00     0.00 10532.00    21.43     0.94    0.95    0.00    0.95   0.95  93.60
    bs=16k	sda               0.00  1057.00    0.00 1058.00     0.00 12696.00    24.00     0.93    0.88    0.00    0.88   0.88  93.10
    bs=32k	sda               0.00   854.00    0.00  876.00     0.00 14812.00    33.82     0.97    1.11    0.00    1.11   1.07  93.50
   bs=128k	sda               0.00   586.00    0.00  584.00     0.00 28736.00    98.41     0.95    1.63    0.00    1.63   1.62  94.60
   bs=512k	sda               0.00   172.00    0.00  169.00     0.00 30308.00   358.67     0.96    5.69    0.00    5.69   5.67  95.90
  bs=1024k	sda               0.00    99.00    1.00  130.00     4.00 33428.00   510.41     1.52   11.67   12.00   11.67   7.34  96.10
  bs=2048k	sda               0.00    60.00    0.00  120.00     0.00 41360.00   689.33     2.56   20.80    0.00   20.80   7.94  95.30
  bs=5120k	sda               0.00    29.00    0.00   95.00     0.00 39108.00   823.33     5.13   51.33    0.00   51.33  10.06  95.60
fio --name=randwrite --rw=randwrite --bs=4k --size=20M --runtime=30 --ioengine=libaio --iodepth=1 --numjobs=1 --filename=/dev/sda --direct=1 --group_reporting
		sda               0.00     0.00    0.00 1033.00     0.00  4132.00     8.00     0.97    0.95    0.00    0.95   0.94  97.50
		sda               0.00     0.00    0.00 2370.00     0.00  9480.00     8.00     9.93    4.14    0.00    4.14   0.42 100.00    #iodepth=10
		sda               0.00     7.00    0.00  225.00     0.00 114212.00  1015.22     0.99    4.43    0.00    4.43   4.36  98.20   #bc=512k  size=200M
		sda               0.00     9.00    0.00   92.00     0.00 46124.00  1002.70     1.49   16.23    0.00   16.23  10.82  99.50    #bc=1024k size=200M
```

#### 推测一： 包大小不变的情况下，TiDB通过加大3倍的并发量，能压倒磁盘性能上限。

#### 推测二： 优化数据包落盘大小。这可能是一个比较复杂的问题，要考虑到update事务保障，即使用只要事务日志落盘不管数据是否落盘的手段，光写事务日志这些事务日志可能也是小包，这里又牵出来group commit的概念了。TiDB做到这程度我只能膜拜了。

#### 手段三：把事务日志放到更高性能的磁盘上去。

```shell
17:45:38 Total DISK READ :     117.47 K/s | Total DISK WRITE :       4.81 M/s
17:45:38 Actual DISK READ:     117.47 K/s | Actual DISK WRITE:    1811.28 K/s
17:45:38   418 be/3 root        0.00 B/s    0.00 B/s  0.00 % 42.33 % [jbd2/sda2-8]
17:45:38  2287 be/4 tidb        0.00 B/s  200.83 K/s  0.00 % 29.80 % bin/tikv-server --addr 0.0.0.0:30161 --advertise-addr 192.168.1.242:30161 --status-addr 192.168.1.242:30181 --pd 192.168.1.242:2379 --data-dir /tidb-data830/tikv-30161 --config conf/tikv.toml --log-file /tidb-deploy830/tikv-30161/log/tikv.log [raftstore-4-0]
17:45:38 23371 be/4 root       72.00 K/s    2.05 M/s  0.00 % 27.41 % perf record -F 99 -p 888 --call-graph=dwarf sleep 30
17:45:38  2288 be/4 tidb        0.00 B/s  219.78 K/s  0.00 % 23.30 % bin/tikv-server --addr 0.0.0.0:30161 --advertise-addr 192.168.1.242:30161 --status-addr 192.168.1.242:30181 --pd 192.168.1.242:2379 --data-dir /tidb-data830/tikv-30161 --config conf/tikv.toml --log-file /tidb-deploy830/tikv-30161/log/tikv.log [raftstore-4-1]
17:45:38 23378 be/4 root       34.10 K/s  174.31 K/s  0.00 %  7.01 % perf record -e probe_tikv:malloc -F 99 -p 888 -g -- sleep 10
17:45:38  2290 be/4 tidb        0.00 B/s   49.26 K/s  0.00 %  2.25 % bin/tikv-server --addr 0.0.0.0:30161 --advertise-addr 192.168.1.242:30161 --status-addr 192.168.1.242:30181 --pd 192.168.1.242:2379 --data-dir /tidb-data830/tikv-30161 --config conf/tikv.toml --log-file /tidb-deploy830/tikv-30161/log/tikv.log [apply-1]
17:45:38 23377 be/4 root       11.37 K/s    2.05 M/s  0.00 %  1.55 % perf record -F 99 -p 888 --call-graph=dwarf sleep 30
17:45:38  2103 be/4 tidb        0.00 B/s    3.79 K/s  0.00 %  1.39 % bin/pd-server --name=pd-192.168.1.242-2379 --client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://192.168.1.242:2379 --peer-urls=http://192.168.1.242:2380 --advertise-peer-urls=http://192.168.1.242:2380 --data-dir=/tidb-data830/pd-2379 --initial-cluster=pd-192.168.1.242-2379=http://192.168.1.242:2380 --config=conf/pd.toml --log-file=/tidb-deploy830/pd-2379/log/pd.log
```



## 下面是并发8，24，72，216，648，800，1200 的测试结果

```shell
[ 10s ] thds: 8 tps: 25.09 qps: 153.16 (r/w/o: 0.00/102.17/50.99) lat (ms,95%): 411.96 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 8 tps: 25.20 qps: 151.31 (r/w/o: 0.00/100.90/50.40) lat (ms,95%): 419.45 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 8 tps: 25.10 qps: 151.10 (r/w/o: 0.00/100.90/50.20) lat (ms,95%): 397.39 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 8 tps: 24.40 qps: 147.00 (r/w/o: 0.00/98.20/48.80) lat (ms,95%): 419.45 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 8 tps: 25.60 qps: 152.80 (r/w/o: 0.00/101.60/51.20) lat (ms,95%): 419.45 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 8 tps: 25.90 qps: 154.80 (r/w/o: 0.00/103.00/51.80) lat (ms,95%): 404.61 err/s: 0.00 reconn/s: 0.00

[ 10s ] thds: 24 tps: 74.83 qps: 456.09 (r/w/o: 0.00/304.03/152.06) lat (ms,95%): 397.39 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 24 tps: 76.60 qps: 460.61 (r/w/o: 0.00/307.41/153.20) lat (ms,95%): 404.61 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 24 tps: 78.80 qps: 473.00 (r/w/o: 0.00/315.40/157.60) lat (ms,95%): 383.33 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 24 tps: 74.30 qps: 444.90 (r/w/o: 0.00/296.30/148.60) lat (ms,95%): 434.83 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 24 tps: 80.00 qps: 480.30 (r/w/o: 0.00/320.30/160.00) lat (ms,95%): 390.30 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 24 tps: 77.40 qps: 465.20 (r/w/o: 0.00/310.50/154.70) lat (ms,95%): 397.39 err/s: 0.00 reconn/s: 0.00

[ 10s ] thds: 72 tps: 208.15 qps: 1270.98 (r/w/o: 0.00/847.48/423.49) lat (ms,95%): 458.96 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 72 tps: 215.31 qps: 1293.75 (r/w/o: 0.00/863.13/430.62) lat (ms,95%): 493.24 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 72 tps: 219.90 qps: 1319.70 (r/w/o: 0.00/879.90/439.80) lat (ms,95%): 467.30 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 72 tps: 211.20 qps: 1263.91 (r/w/o: 0.00/841.50/422.40) lat (ms,95%): 475.79 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 72 tps: 214.90 qps: 1294.39 (r/w/o: 0.00/864.49/429.90) lat (ms,95%): 484.44 err/s: 0.10 reconn/s: 0.00
[ 60s ] thds: 72 tps: 227.80 qps: 1363.80 (r/w/o: 0.00/908.20/455.60) lat (ms,95%): 467.30 err/s: 0.00 reconn/s: 0.00

[ 10s ] thds: 216 tps: 464.84 qps: 2855.33 (r/w/o: 0.00/1904.08/951.24) lat (ms,95%): 746.32 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 216 tps: 504.06 qps: 3025.17 (r/w/o: 0.00/2017.05/1008.12) lat (ms,95%): 719.92 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 216 tps: 492.80 qps: 2957.33 (r/w/o: 0.00/1971.72/985.61) lat (ms,95%): 719.92 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 216 tps: 447.19 qps: 2681.12 (r/w/o: 0.00/1786.95/894.17) lat (ms,95%): 802.05 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 216 tps: 479.81 qps: 2887.37 (r/w/o: 0.00/1927.55/959.82) lat (ms,95%): 746.32 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 216 tps: 528.69 qps: 3162.27 (r/w/o: 0.00/2105.78/1056.49) lat (ms,95%): 669.89 err/s: 0.00 reconn/s: 0.00

[ 10s ] thds: 648 tps: 911.46 qps: 5643.33 (r/w/o: 0.00/3756.35/1886.97) lat (ms,95%): 1401.61 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 648 tps: 960.64 qps: 5775.03 (r/w/o: 0.00/3853.75/1921.28) lat (ms,95%): 1401.61 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 648 tps: 954.01 qps: 5713.87 (r/w/o: 0.00/3806.85/1907.02) lat (ms,95%): 1453.01 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 648 tps: 878.70 qps: 5277.98 (r/w/o: 0.00/3519.68/1758.29) lat (ms,95%): 1589.90 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 648 tps: 922.79 qps: 5533.62 (r/w/o: 0.00/3689.45/1844.17) lat (ms,95%): 1479.41 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 648 tps: 931.60 qps: 5581.39 (r/w/o: 0.00/3728.30/1853.10) lat (ms,95%): 1479.41 err/s: 0.00 reconn/s: 0.00

[ 10s ] thds: 800 tps: 960.16 qps: 5975.54 (r/w/o: 0.00/3979.40/1996.13) lat (ms,95%): 1708.63 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 800 tps: 981.69 qps: 5879.92 (r/w/o: 0.00/3914.75/1965.17) lat (ms,95%): 2082.91 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 800 tps: 953.05 qps: 5725.58 (r/w/o: 0.00/3818.99/1906.59) lat (ms,95%): 2159.29 err/s: 0.10 reconn/s: 0.00
[ 40s ] thds: 800 tps: 931.10 qps: 5583.10 (r/w/o: 0.00/3720.90/1862.20) lat (ms,95%): 2082.91 err/s: 0.10 reconn/s: 0.00
[ 50s ] thds: 800 tps: 936.84 qps: 5639.04 (r/w/o: 0.00/3766.06/1872.98) lat (ms,95%): 1973.38 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 800 tps: 943.59 qps: 5626.02 (r/w/o: 0.00/3753.55/1872.47) lat (ms,95%): 1938.16 err/s: 0.10 reconn/s: 0.00

[ 10s ] thds: 1200 tps: 856.22 qps: 5418.41 (r/w/o: 0.00/3593.08/1825.33) lat (ms,95%): 2932.60 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 1200 tps: 752.02 qps: 4500.21 (r/w/o: 0.00/2995.27/1504.94) lat (ms,95%): 4943.53 err/s: 0.20 reconn/s: 0.00
[ 30s ] thds: 1200 tps: 677.50 qps: 4071.52 (r/w/o: 0.00/2715.21/1356.31) lat (ms,95%): 5918.87 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 1200 tps: 516.80 qps: 3088.29 (r/w/o: 0.00/2053.90/1034.40) lat (ms,95%): 7479.98 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 1200 tps: 434.60 qps: 2608.60 (r/w/o: 0.00/1740.80/867.80) lat (ms,95%): 10531.32 err/s: 0.20 reconn/s: 0.00
[ 60s ] thds: 1200 tps: 447.10 qps: 2673.70 (r/w/o: 0.00/1780.00/893.70) lat (ms,95%): 9977.52 err/s: 0.00 reconn/s: 0.00
```



```shell
Device:   rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util

sda               0.00   153.00    0.00  267.00     0.00  1792.00    13.42     0.96    3.57    0.00    3.57   3.60  96.10
sda               0.00   152.00    0.00  263.00     0.00  2204.00    16.76     1.01    3.92    0.00    3.92   3.75  98.50
sda               0.00   151.00    0.00  255.00     0.00  1720.00    13.49     0.92    3.60    0.00    3.60   3.59  91.60

sda               0.00   189.00    0.00  320.00     0.00  2328.00    14.55     0.98    3.04    0.00    3.04   3.06  97.80
sda               0.00   179.00    0.00  304.00     0.00  2212.00    14.55     0.94    3.11    0.00    3.11   3.11  94.50
sda               0.00   189.00    0.00  322.00     0.00  3512.00    21.81     1.01    3.16    0.00    3.16   2.94  94.80

sda               0.00   192.00    0.00  301.00     0.00  3776.00    25.09     0.96    3.22    0.00    3.22   3.14  94.40
sda               0.00   189.00    0.00  293.00     0.00  4016.00    27.41     0.99    3.40    0.00    3.40   3.25  95.10
sda               0.00   178.00    0.00  286.00     0.00  4252.00    29.73     1.07    3.73    0.00    3.73   3.30  94.30

sda               0.00   210.00    0.00  268.00     0.00  5232.00    39.04     0.94    3.51    0.00    3.51   3.43  92.00
sda               0.00   207.00    0.00  262.00     0.00  4616.00    35.24     0.98    3.69    0.00    3.69   3.55  92.90
sda               0.00   223.00    0.00  260.00     0.00  5480.00    42.15     0.92    3.55    0.00    3.55   3.47  90.30

sda               0.00   206.00    0.00  191.00     0.00  8264.00    86.53     0.92    4.90    0.00    4.90   4.44  84.80
sda               0.00   202.00    0.00  182.00     0.00  8276.00    90.95     1.14    6.24    0.00    6.24   4.78  87.00
sda               0.00   215.00    0.00  208.00     0.00 22544.00   216.77     2.19   10.43    0.00   10.43   4.22  87.80

sda               0.00   244.00    0.00  211.00     0.00  8700.00    82.46     0.82    3.86    0.00    3.86   3.74  79.00
sda               0.00   221.00    0.00  215.00     0.00  8028.00    74.68     0.89    4.19    0.00    4.19   4.00  86.00
sda               0.00   225.00    0.00  203.00     0.00  8768.00    86.38     0.98    4.77    0.00    4.77   4.01  81.50

sda               0.00   242.00    0.00  217.00     0.00  5884.00    54.23     0.94    4.30    0.00    4.30   4.02  87.20
sda               0.00   251.00    0.00  235.00     0.00  6588.00    56.07     0.85    3.63    0.00    3.63   3.50  82.20
sda               0.00   241.00    0.00  222.00     0.00  6368.00    57.37     0.94    4.25    0.00    4.25   4.11  91.30
```



这个结果比我之前的推断的结果还要好，TiDB做的不错，IO的包大小自动越来越大，看来是tidb在代码层面下了功夫优化的，应该是在TiKV层做了类似与group commit的优化。

这么看来IO没有成为瓶颈，我考虑把cpu直接压到100试试。随着并发的增加性能的提升从线性到平缓到停滞到下降。

从Tidb grafana和Tidb dashboard上面也看了一下，这里面还有好多的细节（函数对CPUMEM的使用，GO和RUST对系统的调用，GO对RUST的调用，PD的调度，Range的分裂，SQL到KV的映射，apply，deadlock-detect，grpc-server， raftstore, sched-work-po, waiter-manager, raft-gc, rocksdb 等等）后面一步步来吧 。

